# ============================================
# Imports (aligned with course labs)
# ============================================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
import random

import kagglehub
import scipy.io

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, roc_auc_score

import torch
from torch import nn
from torch.utils.data import TensorDataset, DataLoader

# --------------------------------------------
# Reproducibility helper (lab-style)
# --------------------------------------------
def fix_random_seed(seed: int = 42):
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

fix_random_seed(42)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

plt.rcParams["figure.figsize"] = (10, 4)


# ============================================
# 1. Download & Load Dataset
# ============================================
print("Downloading ECG dataset from Kaggle (radwakandeel/ecg-signals)...")
path = kagglehub.dataset_download("radwakandeel/ecg-signals")
print("Dataset path:", path)

root_dir = Path(path) / "New folder"
print("Root directory for class folders:", root_dir)

print("\nClass folders found:")
for item in root_dir.iterdir():
    if item.is_dir():
        print("  ", item.name)

# -----------------------
# Load .mat files
# -----------------------
data_rows = []

for class_dir in root_dir.iterdir():
    if not class_dir.is_dir():
        continue

    label = class_dir.name
    print("Processing class:", label)

    mat_files = list(class_dir.glob("*.mat"))
    if not mat_files:
        print(f"  No .mat files in {class_dir}")
        continue

    for mat_file in mat_files:
        mat_data = scipy.io.loadmat(mat_file)
        if "val" not in mat_data:
            print(f"  Warning: 'val' key missing in {mat_file}")
            continue

        signal = mat_data["val"].flatten()
        row = {f"s{i}": v for i, v in enumerate(signal)}
        row["label"] = label
        data_rows.append(row)

df = pd.DataFrame(data_rows)
print("\nFinal dataframe shape:", df.shape)
print("First few columns:", df.columns[:10])
print("\nLabel distribution:\n", df["label"].value_counts())


# ============================================
# 2. Binary Labels + EDA
# ============================================
VTACH_LABEL = "10 VT"   # ventricular tachycardia folder

df["is_vtach"] = (df["label"] == VTACH_LABEL).astype(int)
print("\nBinary distribution (0=non-VT, 1=VT):\n", df["is_vtach"].value_counts())

# ---- Plot 1: Class imbalance ----
df["is_vtach"].value_counts().rename({0: "Non-V-tach", 1: "V-tach"}).plot(kind="bar")
plt.title("Class Balance: V-tach vs Non-V-tach")
plt.ylabel("Number of samples")
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

# ---- Plot 2: Example V-tach waveform ----
signal_cols = [c for c in df.columns if c.startswith("s")]
print("Number of signal samples per recording:", len(signal_cols))

if df["is_vtach"].sum() == 0:
    raise ValueError("No V-tach samples found – check VTACH_LABEL.")

vt_example = df[df["is_vtach"] == 1].iloc[0][signal_cols].values
plt.plot(vt_example)
plt.title("Example V-tach ECG Waveform")
plt.xlabel("Sample index")
plt.ylabel("Amplitude (a.u.)")
plt.tight_layout()
plt.show()

# Optional: Example non-VT waveform
nonvt_example = df[df["is_vtach"] == 0].iloc[0][signal_cols].values
plt.plot(nonvt_example)
plt.title("Example Non-V-tach ECG Waveform")
plt.xlabel("Sample index")
plt.ylabel("Amplitude (a.u.)")
plt.tight_layout()
plt.show()


# ============================================
# 3. Preprocessing & Train/Test Split
# ============================================
X = df[signal_cols].values
y = df["is_vtach"].values

# Single outer split: train (70%) / test (30%)
X_train_raw, X_test_raw, y_train, y_test = train_test_split(
    X, y,
    test_size=0.30,
    stratify=y,
    random_state=42
)

# Fit scaler on TRAIN ONLY (avoid test leakage)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train_raw)
X_test = scaler.transform(X_test_raw)

print("\nTrain shape:", X_train.shape)
print("Test shape:",  X_test.shape)
print("Train class counts:", np.bincount(y_train))
print("Test class counts: ", np.bincount(y_test))


# ============================================
# 4. Shared evaluation helper
# ============================================
def evaluate_probabilities(y_true, y_prob, threshold=0.5, split_name="Test"):
    y_pred = (y_prob >= threshold).astype(int)

    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()

    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0   # recall
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    roc_auc = roc_auc_score(y_true, y_prob)

    print(f"\n=== {split_name} Metrics ===")
    print("Confusion matrix:")
    print(np.array([[tn, fp],
                    [fn, tp]]))
    print(f"Sensitivity (Recall): {sensitivity:.3f}")
    print(f"Specificity:         {specificity:.3f}")
    print(f"False Positive Rate: {fpr:.3f}")
    print(f"Precision:           {precision:.3f}")
    print(f"ROC-AUC:             {roc_auc:.3f}")

    return {
        "sensitivity": sensitivity,
        "specificity": specificity,
        "fpr": fpr,
        "precision": precision,
        "roc_auc": roc_auc,
    }


# ============================================
# 5. Logistic Regression with k-fold CV
# ============================================
print("\n=== Logistic Regression: 5-fold Stratified CV on TRAIN set ===")

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_metrics = []

for fold, (tr_idx, val_idx) in enumerate(skf.split(X_train, y_train), start=1):
    X_tr, X_val = X_train[tr_idx], X_train[val_idx]
    y_tr, y_val = y_train[tr_idx], y_train[val_idx]

    # Handle imbalance via class_weight='balanced' (covered conceptually in lectures on rare events)
    logreg = LogisticRegression(
        max_iter=1000,
        class_weight="balanced",
        random_state=42
    )
    logreg.fit(X_tr, y_tr)

    y_val_prob = logreg.predict_proba(X_val)[:, 1]
    print(f"\nFold {fold}")
    metrics = evaluate_probabilities(y_val, y_val_prob, split_name=f"CV Fold {fold}")
    cv_metrics.append(metrics)

# Summarize CV metrics
cv_summary = {
    key: np.mean([m[key] for m in cv_metrics]) for key in cv_metrics[0].keys()
}
print("\n=== Mean CV metrics across 5 folds (TRAIN only) ===")
for k, v in cv_summary.items():
    print(f"{k}: {v:.3f}")

# Train final LogReg on full TRAIN set and evaluate on held-out TEST
final_logreg = LogisticRegression(
    max_iter=1000,
    class_weight="balanced",
    random_state=42
)
final_logreg.fit(X_train, y_train)
y_test_prob_lr = final_logreg.predict_proba(X_test)[:, 1]
eval_lr_test = evaluate_probabilities(y_test, y_test_prob_lr, split_name="Test (LogReg)")


# ============================================
# 6. 1D CNN (PyTorch) – Deep model from lectures
# ============================================

# Further split TRAIN into train/validation for CNN
X_train_cnn_raw, X_val_cnn_raw, y_train_cnn, y_val_cnn = train_test_split(
    X_train, y_train,
    test_size=0.2,
    stratify=y_train,
    random_state=42
)

# Convert to tensors
X_train_cnn_t = torch.as_tensor(X_train_cnn_raw, dtype=torch.float32).to(device)
y_train_cnn_t = torch.as_tensor(y_train_cnn, dtype=torch.float32).to(device)

X_val_cnn_t = torch.as_tensor(X_val_cnn_raw, dtype=torch.float32).to(device)
y_val_cnn_t = torch.as_tensor(y_val_cnn, dtype=torch.float32).to(device)

X_test_t = torch.as_tensor(X_test, dtype=torch.float32).to(device)
y_test_t = torch.as_tensor(y_test, dtype=torch.float32).to(device)

train_dataset_cnn = TensorDataset(X_train_cnn_t, y_train_cnn_t)
train_loader_cnn = DataLoader(train_dataset_cnn, batch_size=32, shuffle=True)

# Compute pos_weight for imbalanced BCE loss (N_neg / N_pos)
pos_count = y_train_cnn.sum()
neg_count = len(y_train_cnn) - pos_count
pos_weight_value = (neg_count / pos_count) if pos_count > 0 else 1.0
print("\nCNN pos_weight (N_neg / N_pos):", pos_weight_value)


class CNN1DClassifier(nn.Module):
    """
    Simple 1D CNN classifier, aligned with Lecture 3 & 5:
    - Conv1D + ReLU + MaxPool
    - Conv1D + ReLU + MaxPool
    - Flatten + Linear -> 1 logit
    """
    def __init__(self, input_length: int):
        super().__init__()
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=5, padding=2)
        self.relu1 = nn.ReLU()
        self.pool1 = nn.MaxPool1d(kernel_size=2)

        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, padding=2)
        self.relu2 = nn.ReLU()
        self.pool2 = nn.MaxPool1d(kernel_size=2)

        pooled_length = input_length // 4
        self.fc = nn.Linear(32 * pooled_length, 1)

    def forward(self, x):
        # x: (B, T) -> (B, 1, T)
        if x.ndim == 2:
            x = x.unsqueeze(1)
        x = self.conv1(x)
        x = self.relu1(x)
        x = self.pool1(x)

        x = self.conv2(x)
        x = self.relu2(x)
        x = self.pool2(x)

        x = x.flatten(start_dim=1)
        logits = self.fc(x)
        return logits.squeeze(1)


input_length = X_train.shape[1]
cnn_model = CNN1DClassifier(input_length=input_length).to(device)

pos_weight = torch.tensor([pos_weight_value], dtype=torch.float32).to(device)
criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
optimizer = torch.optim.Adam(cnn_model.parameters(), lr=1e-3)

print("\nCNN model:")
print(cnn_model)

# -----------------------
# Train CNN
# -----------------------
nr_epochs = 20
train_losses = []
val_losses = []

for epoch in range(nr_epochs):
    cnn_model.train()
    epoch_loss = 0.0

    for batch_X, batch_y in train_loader_cnn:
        optimizer.zero_grad()
        logits = cnn_model(batch_X)
        loss = criterion(logits, batch_y)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item() * batch_X.size(0)

    epoch_loss /= len(train_dataset_cnn)
    train_losses.append(epoch_loss)

    cnn_model.eval()
    with torch.no_grad():
        val_logits = cnn_model(X_val_cnn_t)
        val_loss = criterion(val_logits, y_val_cnn_t).item()
        val_losses.append(val_loss)

    print(f"Epoch {epoch+1:02d}/{nr_epochs} - "
          f"Train loss: {epoch_loss:.4f} - Val loss: {val_loss:.4f}")

# Plot CNN training curve (optional figure for report)
plt.plot(train_losses, label="Train loss")
plt.plot(val_losses, label="Validation loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("CNN Training & Validation Loss")
plt.legend()
plt.tight_layout()
plt.show()


# -----------------------
# Evaluate CNN on val + test with same metrics
# -----------------------
def cnn_predict_proba(model, X_t):
    model.eval()
    with torch.no_grad():
        logits = model(X_t)
        probs = torch.sigmoid(logits).cpu().numpy()
    return probs

print("\nCNN RESULTS")
y_val_prob_cnn = cnn_predict_proba(cnn_model, X_val_cnn_t)
y_test_prob_cnn = cnn_predict_proba(cnn_model, X_test_t)

_ = evaluate_probabilities(y_val_cnn, y_val_prob_cnn, split_name="Validation (CNN)")
eval_cnn_test = evaluate_probabilities(y_test, y_test_prob_cnn, split_name="Test (CNN)")


# ============================================
# 7. Summary Comparison (Test set)
# ============================================
print("\n=== Test Set Summary ===")
print("Logistic Regression (Test):", eval_lr_test)
print("CNN 1D (Test):             ", eval_cnn_test)
